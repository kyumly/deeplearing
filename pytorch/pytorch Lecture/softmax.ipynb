{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "y = np.array([1,0,0])\n",
    "\n",
    "y_pred1 = np.array([0.7, 0.2, 0.1])\n",
    "y_pred2 = np.array([0.1, 0.3, 0.6])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  0.35667494393873245\n",
      "loss :  2.3025850929940455\n"
     ]
    }
   ],
   "source": [
    "print(\"loss : \",np.sum(-y * np.log(y_pred1)))\n",
    "print(\"loss : \",np.sum(-y * np.log(y_pred2)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "y =Variable(torch.LongTensor([0]))\n",
    "y.requires_grad = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "y_pred1 = torch.Tensor([[2.0,1.0, 0.1]])\n",
    "y_pred2 = torch.Tensor([[0.5,2.0, 0.3]])\n",
    "\n",
    "l1 = loss(y_pred1, y)\n",
    "l2 = loss(y_pred2, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss1 :  tensor(0.4170) \n",
      " loss2 :  tensor(1.8406)\n"
     ]
    }
   ],
   "source": [
    "print(\"loss1 : \", l1.data,\"\\n\",\n",
    "      \"loss2 : \", l2.data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss1 :  tensor(0.4966) \n",
      " loss2 :  tensor(1.2389)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "y = Variable(torch.LongTensor([2,0,1]), requires_grad = False)\n",
    "\n",
    "y_pred1 = Variable(torch.Tensor(\n",
    "    [\n",
    "        [0.1, 0.2, 0.9],\n",
    "        [1.1, 0.1, 0.2],\n",
    "        [0.2, 2.1, 0.1]\n",
    "    ]\n",
    "))\n",
    "\n",
    "y_pred2 = Variable(torch.Tensor(\n",
    "    [\n",
    "        [0.8, 0.2, 0.3],\n",
    "        [0.2, 0.3, 0.5],\n",
    "        [0.2, 0.2, 0.5]\n",
    "    ]\n",
    "))\n",
    "\n",
    "l1 = loss(y_pred1, y)\n",
    "l2 = loss(y_pred2 , y)\n",
    "\n",
    "print(\"loss1 : \", l1.data,\"\\n\",\n",
    "      \"loss2 : \", l2.data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "import torch.nn.functional as f\n",
    "\n",
    "class net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(784, 520)\n",
    "        self.l2 = nn.Linear(520, 320)\n",
    "        self.l3 = nn.Linear(320, 240)\n",
    "        self.l4 = nn.Linear(240, 120)\n",
    "        self.l5 = nn.Linear(120, 10)\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = f.relu(self.l1(x))\n",
    "        x = f.relu(self.l2(x))\n",
    "        x = f.relu(self.l3(x))\n",
    "        x = f.relu(self.l4(x))\n",
    "        x = f.softmax(self.l5(x))\n",
    "\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils import data\n",
    "batch_size = 64\n",
    "train_dataset = datasets.MNIST(root='./data/',\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./data/',\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "model = net()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(), lr=0.01\n",
    ")\n",
    "#optim = torch.optim.SGD(model.parameters(), lr=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target  = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v5/xvhr16_s7qd6nfvqvstsw2h40000gp/T/ipykernel_933/2759191271.py:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = f.softmax(self.l5(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 | Batch Status: 0/60000 (0%) | Loss: 2.302647\n",
      "Train Epoch: 1 | Batch Status: 640/60000 (1%) | Loss: 2.302430\n",
      "Train Epoch: 1 | Batch Status: 1280/60000 (2%) | Loss: 2.301974\n",
      "Train Epoch: 1 | Batch Status: 1920/60000 (3%) | Loss: 2.302667\n",
      "Train Epoch: 1 | Batch Status: 2560/60000 (4%) | Loss: 2.301945\n",
      "Train Epoch: 1 | Batch Status: 3200/60000 (5%) | Loss: 2.300351\n",
      "Train Epoch: 1 | Batch Status: 3840/60000 (6%) | Loss: 2.302113\n",
      "Train Epoch: 1 | Batch Status: 4480/60000 (7%) | Loss: 2.302340\n",
      "Train Epoch: 1 | Batch Status: 5120/60000 (9%) | Loss: 2.302938\n",
      "Train Epoch: 1 | Batch Status: 5760/60000 (10%) | Loss: 2.303535\n",
      "Train Epoch: 1 | Batch Status: 6400/60000 (11%) | Loss: 2.302216\n",
      "Train Epoch: 1 | Batch Status: 7040/60000 (12%) | Loss: 2.302862\n",
      "Train Epoch: 1 | Batch Status: 7680/60000 (13%) | Loss: 2.303107\n",
      "Train Epoch: 1 | Batch Status: 8320/60000 (14%) | Loss: 2.302387\n",
      "Train Epoch: 1 | Batch Status: 8960/60000 (15%) | Loss: 2.302207\n",
      "Train Epoch: 1 | Batch Status: 9600/60000 (16%) | Loss: 2.302802\n",
      "Train Epoch: 1 | Batch Status: 10240/60000 (17%) | Loss: 2.302470\n",
      "Train Epoch: 1 | Batch Status: 10880/60000 (18%) | Loss: 2.302704\n",
      "Train Epoch: 1 | Batch Status: 11520/60000 (19%) | Loss: 2.303171\n",
      "Train Epoch: 1 | Batch Status: 12160/60000 (20%) | Loss: 2.302012\n",
      "Train Epoch: 1 | Batch Status: 12800/60000 (21%) | Loss: 2.303211\n",
      "Train Epoch: 1 | Batch Status: 13440/60000 (22%) | Loss: 2.302011\n",
      "Train Epoch: 1 | Batch Status: 14080/60000 (23%) | Loss: 2.302435\n",
      "Train Epoch: 1 | Batch Status: 14720/60000 (25%) | Loss: 2.302424\n",
      "Train Epoch: 1 | Batch Status: 15360/60000 (26%) | Loss: 2.302951\n",
      "Train Epoch: 1 | Batch Status: 16000/60000 (27%) | Loss: 2.303319\n",
      "Train Epoch: 1 | Batch Status: 16640/60000 (28%) | Loss: 2.301604\n",
      "Train Epoch: 1 | Batch Status: 17280/60000 (29%) | Loss: 2.302481\n",
      "Train Epoch: 1 | Batch Status: 17920/60000 (30%) | Loss: 2.302154\n",
      "Train Epoch: 1 | Batch Status: 18560/60000 (31%) | Loss: 2.301831\n",
      "Train Epoch: 1 | Batch Status: 19200/60000 (32%) | Loss: 2.302149\n",
      "Train Epoch: 1 | Batch Status: 19840/60000 (33%) | Loss: 2.302816\n",
      "Train Epoch: 1 | Batch Status: 20480/60000 (34%) | Loss: 2.302472\n",
      "Train Epoch: 1 | Batch Status: 21120/60000 (35%) | Loss: 2.303187\n",
      "Train Epoch: 1 | Batch Status: 21760/60000 (36%) | Loss: 2.302400\n",
      "Train Epoch: 1 | Batch Status: 22400/60000 (37%) | Loss: 2.302016\n",
      "Train Epoch: 1 | Batch Status: 23040/60000 (38%) | Loss: 2.302592\n",
      "Train Epoch: 1 | Batch Status: 23680/60000 (39%) | Loss: 2.302784\n",
      "Train Epoch: 1 | Batch Status: 24320/60000 (41%) | Loss: 2.303262\n",
      "Train Epoch: 1 | Batch Status: 24960/60000 (42%) | Loss: 2.302736\n",
      "Train Epoch: 1 | Batch Status: 25600/60000 (43%) | Loss: 2.301474\n",
      "Train Epoch: 1 | Batch Status: 26240/60000 (44%) | Loss: 2.302822\n",
      "Train Epoch: 1 | Batch Status: 26880/60000 (45%) | Loss: 2.304272\n",
      "Train Epoch: 1 | Batch Status: 27520/60000 (46%) | Loss: 2.302367\n",
      "Train Epoch: 1 | Batch Status: 28160/60000 (47%) | Loss: 2.302305\n",
      "Train Epoch: 1 | Batch Status: 28800/60000 (48%) | Loss: 2.302575\n",
      "Train Epoch: 1 | Batch Status: 29440/60000 (49%) | Loss: 2.302535\n",
      "Train Epoch: 1 | Batch Status: 30080/60000 (50%) | Loss: 2.301729\n",
      "Train Epoch: 1 | Batch Status: 30720/60000 (51%) | Loss: 2.302230\n",
      "Train Epoch: 1 | Batch Status: 31360/60000 (52%) | Loss: 2.303579\n",
      "Train Epoch: 1 | Batch Status: 32000/60000 (53%) | Loss: 2.303015\n",
      "Train Epoch: 1 | Batch Status: 32640/60000 (54%) | Loss: 2.303443\n",
      "Train Epoch: 1 | Batch Status: 33280/60000 (55%) | Loss: 2.302464\n",
      "Train Epoch: 1 | Batch Status: 33920/60000 (57%) | Loss: 2.302266\n",
      "Train Epoch: 1 | Batch Status: 34560/60000 (58%) | Loss: 2.302565\n",
      "Train Epoch: 1 | Batch Status: 35200/60000 (59%) | Loss: 2.302519\n",
      "Train Epoch: 1 | Batch Status: 35840/60000 (60%) | Loss: 2.302698\n",
      "Train Epoch: 1 | Batch Status: 36480/60000 (61%) | Loss: 2.301985\n",
      "Train Epoch: 1 | Batch Status: 37120/60000 (62%) | Loss: 2.302656\n",
      "Train Epoch: 1 | Batch Status: 37760/60000 (63%) | Loss: 2.302358\n",
      "Train Epoch: 1 | Batch Status: 38400/60000 (64%) | Loss: 2.302795\n",
      "Train Epoch: 1 | Batch Status: 39040/60000 (65%) | Loss: 2.302205\n",
      "Train Epoch: 1 | Batch Status: 39680/60000 (66%) | Loss: 2.302811\n",
      "Train Epoch: 1 | Batch Status: 40320/60000 (67%) | Loss: 2.302053\n",
      "Train Epoch: 1 | Batch Status: 40960/60000 (68%) | Loss: 2.301877\n",
      "Train Epoch: 1 | Batch Status: 41600/60000 (69%) | Loss: 2.302373\n",
      "Train Epoch: 1 | Batch Status: 42240/60000 (70%) | Loss: 2.302789\n",
      "Train Epoch: 1 | Batch Status: 42880/60000 (71%) | Loss: 2.303282\n",
      "Train Epoch: 1 | Batch Status: 43520/60000 (72%) | Loss: 2.302424\n",
      "Train Epoch: 1 | Batch Status: 44160/60000 (74%) | Loss: 2.302197\n",
      "Train Epoch: 1 | Batch Status: 44800/60000 (75%) | Loss: 2.302849\n",
      "Train Epoch: 1 | Batch Status: 45440/60000 (76%) | Loss: 2.301900\n",
      "Train Epoch: 1 | Batch Status: 46080/60000 (77%) | Loss: 2.303059\n",
      "Train Epoch: 1 | Batch Status: 46720/60000 (78%) | Loss: 2.303205\n",
      "Train Epoch: 1 | Batch Status: 47360/60000 (79%) | Loss: 2.302680\n",
      "Train Epoch: 1 | Batch Status: 48000/60000 (80%) | Loss: 2.303391\n",
      "Train Epoch: 1 | Batch Status: 48640/60000 (81%) | Loss: 2.302830\n",
      "Train Epoch: 1 | Batch Status: 49280/60000 (82%) | Loss: 2.302918\n",
      "Train Epoch: 1 | Batch Status: 49920/60000 (83%) | Loss: 2.301948\n",
      "Train Epoch: 1 | Batch Status: 50560/60000 (84%) | Loss: 2.302191\n",
      "Train Epoch: 1 | Batch Status: 51200/60000 (85%) | Loss: 2.302419\n",
      "Train Epoch: 1 | Batch Status: 51840/60000 (86%) | Loss: 2.301902\n",
      "Train Epoch: 1 | Batch Status: 52480/60000 (87%) | Loss: 2.302902\n",
      "Train Epoch: 1 | Batch Status: 53120/60000 (88%) | Loss: 2.302311\n",
      "Train Epoch: 1 | Batch Status: 53760/60000 (90%) | Loss: 2.302781\n",
      "Train Epoch: 1 | Batch Status: 54400/60000 (91%) | Loss: 2.303482\n",
      "Train Epoch: 1 | Batch Status: 55040/60000 (92%) | Loss: 2.303009\n",
      "Train Epoch: 1 | Batch Status: 55680/60000 (93%) | Loss: 2.302440\n",
      "Train Epoch: 1 | Batch Status: 56320/60000 (94%) | Loss: 2.302693\n",
      "Train Epoch: 1 | Batch Status: 56960/60000 (95%) | Loss: 2.302847\n",
      "Train Epoch: 1 | Batch Status: 57600/60000 (96%) | Loss: 2.302376\n",
      "Train Epoch: 1 | Batch Status: 58240/60000 (97%) | Loss: 2.302202\n",
      "Train Epoch: 1 | Batch Status: 58880/60000 (98%) | Loss: 2.303344\n",
      "Train Epoch: 1 | Batch Status: 59520/60000 (99%) | Loss: 2.304006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v5/xvhr16_s7qd6nfvqvstsw2h40000gp/T/ipykernel_933/2661621735.py:6: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'size_average'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [76]\u001B[0m, in \u001B[0;36m<cell line: 17>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m10\u001B[39m):\n\u001B[1;32m     18\u001B[0m     train(epoch)\n\u001B[0;32m---> 19\u001B[0m     \u001B[43mtest\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [76]\u001B[0m, in \u001B[0;36mtest\u001B[0;34m()\u001B[0m\n\u001B[1;32m      6\u001B[0m data, target \u001B[38;5;241m=\u001B[39m Variable(data, volatile\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m), Variable(target)\n\u001B[1;32m      7\u001B[0m output \u001B[38;5;241m=\u001B[39m model(data)\n\u001B[0;32m----> 9\u001B[0m test_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mcriterion\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize_average\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mdata[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m     10\u001B[0m pred \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39mmax(\u001B[38;5;241m1\u001B[39m, keepdim \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m)[\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m     11\u001B[0m correct \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m pred\u001B[38;5;241m.\u001B[39meq(target\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mview_as(pred))\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39msum()\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/ml/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "\u001B[0;31mTypeError\u001B[0m: forward() got an unexpected keyword argument 'size_average'"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "\n",
    "        test_loss += criterion(output, target, size_average=False).data[0]\n",
    "        pred = output.max(1, keepdim = True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "for epoch in range(1, 10):\n",
    "    train(epoch)\n",
    "    test()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "60000"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader.dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0706, 0.0706, 0.0706,\n           0.4941, 0.5333, 0.6863, 0.1020, 0.6510, 1.0000, 0.9686, 0.4980,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.1176, 0.1412, 0.3686, 0.6039, 0.6667, 0.9922, 0.9922, 0.9922,\n           0.9922, 0.9922, 0.8824, 0.6745, 0.9922, 0.9490, 0.7647, 0.2510,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1922,\n           0.9333, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n           0.9922, 0.9843, 0.3647, 0.3216, 0.3216, 0.2196, 0.1529, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706,\n           0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.7137,\n           0.9686, 0.9451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.3137, 0.6118, 0.4196, 0.9922, 0.9922, 0.8039, 0.0431, 0.0000,\n           0.1686, 0.6039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0549, 0.0039, 0.6039, 0.9922, 0.3529, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.5451, 0.9922, 0.7451, 0.0078, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0431, 0.7451, 0.9922, 0.2745, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.1373, 0.9451, 0.8824, 0.6275,\n           0.4235, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3176, 0.9412, 0.9922,\n           0.9922, 0.4667, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.7294,\n           0.9922, 0.9922, 0.5882, 0.1059, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627,\n           0.3647, 0.9882, 0.9922, 0.7333, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.9765, 0.9922, 0.9765, 0.2510, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1804, 0.5098,\n           0.7176, 0.9922, 0.9922, 0.8118, 0.0078, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.5804, 0.8980, 0.9922,\n           0.9922, 0.9922, 0.9804, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0941, 0.4471, 0.8667, 0.9922, 0.9922, 0.9922,\n           0.9922, 0.7882, 0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0902, 0.2588, 0.8353, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765,\n           0.3176, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.6706,\n           0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.7647, 0.3137, 0.0353,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.6745, 0.8863, 0.9922,\n           0.9922, 0.9922, 0.9922, 0.9569, 0.5216, 0.0431, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.5333, 0.9922, 0.9922, 0.9922,\n           0.8314, 0.5294, 0.5176, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000]]]),\n 5)"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset MNIST\n    Number of datapoints: 60000\n    Root location: ./data/\n    Split: Train\n    StandardTransform\nTransform: ToTensor()"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}