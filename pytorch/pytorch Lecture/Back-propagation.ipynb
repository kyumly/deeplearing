{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0,4.0, 6.0]\n",
    "\n",
    "w = Variable(torch.Tensor([1.0]), requires_grad =True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "x = Variable(torch.randn(1, 10))\n",
    "prev_h = Variable(torch.randn(1,20))\n",
    "w_h = Variable(torch.randn(20, 20))\n",
    "w_x = Variable(torch.randn(20, 10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "i2h = torch.mm(w_x, x.t())\n",
    "h2h = torch.mm(w_h, prev_h.t())\n",
    "\n",
    "next_h = i2h + h2h\n",
    "next_h = next_h.tanh()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ㅇ"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [15]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mnext_h\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mones\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/ml/lib/python3.9/site-packages/torch/_tensor.py:396\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    387\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    388\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    389\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    390\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    394\u001B[0m         create_graph\u001B[38;5;241m=\u001B[39mcreate_graph,\n\u001B[1;32m    395\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs)\n\u001B[0;32m--> 396\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/ml/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    168\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    170\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    171\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    172\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 173\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    175\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "next_h.backward(torch.ones(20,1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict (before training) 4 tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "def forward(x):\n",
    "    return x * w\n",
    "\n",
    "def loss(x, y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred - y)\n",
    "\n",
    "def se(error):\n",
    "    return error ** 2\n",
    "\n",
    "print('predict (before training)', 4, forward(4).data[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "순간 변화량 :  tensor([-0.0976])\n",
      "\t grad :  1.0 2.0 tensor(-0.0976)\n",
      "순간 변화량 :  tensor([-0.3825])\n",
      "\t grad :  2.0 4.0 tensor(-0.3825)\n",
      "순간 변화량 :  tensor([-0.7917])\n",
      "\t grad :  3.0 6.0 tensor(-0.7917)\n",
      "progress :  0 tensor(0.0174)\n",
      "순간 변화량 :  tensor([-0.0721])\n",
      "\t grad :  1.0 2.0 tensor(-0.0721)\n",
      "순간 변화량 :  tensor([-0.2828])\n",
      "\t grad :  2.0 4.0 tensor(-0.2828)\n",
      "순간 변화량 :  tensor([-0.5853])\n",
      "\t grad :  3.0 6.0 tensor(-0.5853)\n",
      "progress :  1 tensor(0.0095)\n",
      "순간 변화량 :  tensor([-0.0533])\n",
      "\t grad :  1.0 2.0 tensor(-0.0533)\n",
      "순간 변화량 :  tensor([-0.2090])\n",
      "\t grad :  2.0 4.0 tensor(-0.2090)\n",
      "순간 변화량 :  tensor([-0.4327])\n",
      "\t grad :  3.0 6.0 tensor(-0.4327)\n",
      "progress :  2 tensor(0.0052)\n",
      "순간 변화량 :  tensor([-0.0394])\n",
      "\t grad :  1.0 2.0 tensor(-0.0394)\n",
      "순간 변화량 :  tensor([-0.1546])\n",
      "\t grad :  2.0 4.0 tensor(-0.1546)\n",
      "순간 변화량 :  tensor([-0.3199])\n",
      "\t grad :  3.0 6.0 tensor(-0.3199)\n",
      "progress :  3 tensor(0.0028)\n",
      "순간 변화량 :  tensor([-0.0291])\n",
      "\t grad :  1.0 2.0 tensor(-0.0291)\n",
      "순간 변화량 :  tensor([-0.1143])\n",
      "\t grad :  2.0 4.0 tensor(-0.1143)\n",
      "순간 변화량 :  tensor([-0.2365])\n",
      "\t grad :  3.0 6.0 tensor(-0.2365)\n",
      "progress :  4 tensor(0.0016)\n",
      "순간 변화량 :  tensor([-0.0215])\n",
      "\t grad :  1.0 2.0 tensor(-0.0215)\n",
      "순간 변화량 :  tensor([-0.0845])\n",
      "\t grad :  2.0 4.0 tensor(-0.0845)\n",
      "순간 변화량 :  tensor([-0.1749])\n",
      "\t grad :  3.0 6.0 tensor(-0.1749)\n",
      "progress :  5 tensor(0.0008)\n",
      "순간 변화량 :  tensor([-0.0159])\n",
      "\t grad :  1.0 2.0 tensor(-0.0159)\n",
      "순간 변화량 :  tensor([-0.0625])\n",
      "\t grad :  2.0 4.0 tensor(-0.0625)\n",
      "순간 변화량 :  tensor([-0.1293])\n",
      "\t grad :  3.0 6.0 tensor(-0.1293)\n",
      "progress :  6 tensor(0.0005)\n",
      "순간 변화량 :  tensor([-0.0118])\n",
      "\t grad :  1.0 2.0 tensor(-0.0118)\n",
      "순간 변화량 :  tensor([-0.0462])\n",
      "\t grad :  2.0 4.0 tensor(-0.0462)\n",
      "순간 변화량 :  tensor([-0.0956])\n",
      "\t grad :  3.0 6.0 tensor(-0.0956)\n",
      "progress :  7 tensor(0.0003)\n",
      "순간 변화량 :  tensor([-0.0087])\n",
      "\t grad :  1.0 2.0 tensor(-0.0087)\n",
      "순간 변화량 :  tensor([-0.0341])\n",
      "\t grad :  2.0 4.0 tensor(-0.0341)\n",
      "순간 변화량 :  tensor([-0.0707])\n",
      "\t grad :  3.0 6.0 tensor(-0.0707)\n",
      "progress :  8 tensor(0.0001)\n",
      "순간 변화량 :  tensor([-0.0064])\n",
      "\t grad :  1.0 2.0 tensor(-0.0064)\n",
      "순간 변화량 :  tensor([-0.0252])\n",
      "\t grad :  2.0 4.0 tensor(-0.0252)\n",
      "순간 변화량 :  tensor([-0.0522])\n",
      "\t grad :  3.0 6.0 tensor(-0.0522)\n",
      "progress :  9 tensor(7.5804e-05)\n",
      "predict (after training) 4 tensor(7.9905)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    for x_val, y_val in zip(x_data, y_data):\n",
    "        l = se(loss(x_val, y_val))\n",
    "        l.backward()\n",
    "        print('순간 변화량 : ',w.grad)\n",
    "        print('\\t grad : ', x_val, y_val, w.grad.data[0])\n",
    "        w.data = w.data - 0.01 * w.grad.data\n",
    "\n",
    "        w.grad.data.zero_()\n",
    "\n",
    "    print('progress : ' ,epoch, l.data[0])\n",
    "\n",
    "\n",
    "print('predict (after training)', 4, forward(4).data[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t grad :  1.0 2.0 tensor(-0.0002)\n",
      "\t grad :  2.0 4.0 tensor(-0.0009)\n",
      "\t grad :  3.0 6.0 tensor(-0.0019)\n",
      "progress :  0 tensor(9.8744e-08)\n",
      "\t grad :  1.0 2.0 tensor(-0.0002)\n",
      "\t grad :  2.0 4.0 tensor(-0.0007)\n",
      "\t grad :  3.0 6.0 tensor(-0.0014)\n",
      "progress :  1 tensor(5.4148e-08)\n",
      "\t grad :  1.0 2.0 tensor(-0.0001)\n",
      "\t grad :  2.0 4.0 tensor(-0.0005)\n",
      "\t grad :  3.0 6.0 tensor(-0.0010)\n",
      "progress :  2 tensor(2.9468e-08)\n",
      "\t grad :  1.0 2.0 tensor(-9.3937e-05)\n",
      "\t grad :  2.0 4.0 tensor(-0.0004)\n",
      "\t grad :  3.0 6.0 tensor(-0.0008)\n",
      "progress :  3 tensor(1.6088e-08)\n",
      "\t grad :  1.0 2.0 tensor(-6.9380e-05)\n",
      "\t grad :  2.0 4.0 tensor(-0.0003)\n",
      "\t grad :  3.0 6.0 tensor(-0.0006)\n",
      "progress :  4 tensor(8.7348e-09)\n",
      "\t grad :  1.0 2.0 tensor(-5.1260e-05)\n",
      "\t grad :  2.0 4.0 tensor(-0.0002)\n",
      "\t grad :  3.0 6.0 tensor(-0.0004)\n",
      "progress :  5 tensor(4.8467e-09)\n",
      "\t grad :  1.0 2.0 tensor(-3.7909e-05)\n",
      "\t grad :  2.0 4.0 tensor(-0.0001)\n",
      "\t grad :  3.0 6.0 tensor(-0.0003)\n",
      "progress :  6 tensor(2.6521e-09)\n",
      "\t grad :  1.0 2.0 tensor(-2.8133e-05)\n",
      "\t grad :  2.0 4.0 tensor(-0.0001)\n",
      "\t grad :  3.0 6.0 tensor(-0.0002)\n",
      "progress :  7 tensor(1.4552e-09)\n",
      "\t grad :  1.0 2.0 tensor(-2.0981e-05)\n",
      "\t grad :  2.0 4.0 tensor(-8.2016e-05)\n",
      "\t grad :  3.0 6.0 tensor(-0.0002)\n",
      "progress :  8 tensor(7.9149e-10)\n",
      "\t grad :  1.0 2.0 tensor(-1.5497e-05)\n",
      "\t grad :  2.0 4.0 tensor(-6.1035e-05)\n",
      "\t grad :  3.0 6.0 tensor(-0.0001)\n",
      "progress :  9 tensor(4.4020e-10)\n",
      "predict (after training) 4 tensor(8.0000)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    for x_val, y_val in zip(x_data, y_data):\n",
    "        l = se(loss(x_val, y_val))\n",
    "        l.backward()\n",
    "\n",
    "        print('\\t grad : ', x_val, y_val, w.grad.data[0])\n",
    "        w.data = w.data - 0.01 * w.grad.data\n",
    "\n",
    "        w.grad.data.zero_()\n",
    "\n",
    "    print('progress : ' ,epoch, l.data[0])\n",
    "\n",
    "\n",
    "print('predict (after training)', 4, forward(4).data[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}